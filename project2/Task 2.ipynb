{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7441b5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9849218c50>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from datetime import datetime\n",
    "from typing import Sequence\n",
    "from torch.utils.data import random_split\n",
    "from d2l import torch as d2l\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4727859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"localization_train.pt\") \n",
    "val_data = torch.load(\"localization_val.pt\")\n",
    "test_data= torch.load(\"localization_test.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9c3496a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    \n",
    "    # We'll store there the training loss for each epoch\n",
    "    losses_train = []\n",
    "    \n",
    "    # Set the network in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Re-initialize gradients, just in case the model has been inappropriately \n",
    "    # manipulated before the training\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        \n",
    "        # Training loss for the current epoch\n",
    "        loss_train = 0\n",
    "        \n",
    "\n",
    "\n",
    "        # Loop over our dataset (in batches the data loader creates for us)\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            \n",
    "            #print(\"hello\")\n",
    "            \n",
    "            # Feed a batch into our model\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            # Compute the loss we wish to minimize \n",
    "            # Note that by default, it is the mean loss that is computed\n",
    "            # (so entire_batch_loss / batch_size)\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(labels, outputs)\n",
    "        \n",
    "                \n",
    "            #print(loss)\n",
    "            \n",
    "            # Perform the backward step. That is, compute the gradients of all parameters we want the network to learn\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the model\n",
    "            optimizer.step() \n",
    "            \n",
    "            # Zero out gradients before the next round (or the end of training)\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            # Update loss for this epoch\n",
    "            # It is important to transform the loss to a number with .item()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        # Store current epoch loss. \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "        \n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "            \n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c88aaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # to inherit the '__init__' method from the 'nn.Module' class\n",
    "        # Add whatever you want here (e.g layers and activation functions)\n",
    "        # The order and names don't matter here but it is easier to understand\n",
    "        # if you go for Layer1, fun1, layer2, fun2, etc\n",
    "        # Some conventions:\n",
    "        # - conv stands for convolution\n",
    "        # - pool for pooling\n",
    "        # - fc for fully connected\n",
    "\n",
    "        # 32*32*3: determined by our dataset: 32x32 RGB images\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5, stride = 1, padding = 0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = 5, stride = 1, padding = 0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 120, kernel_size = 5, stride = 1, padding = 0)\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = 5*8*120 , out_features = 1000) # 5 is height 8 is width and 120 is nr channels.\n",
    "        self.fc2 = nn.Linear(in_features = 1000 , out_features = 100)\n",
    "        self.fc3 = nn.Linear(in_features = 100 , out_features = 10 +5) #C + 5 components\n",
    "         \n",
    "        \n",
    "             \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Now the order matters! \n",
    "        out = F.relu(self.conv1(x)) #F.relu is the activation function\n",
    "        out = self.pool1(out)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = self.pool2(out)\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = self.flat(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "77663b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    \n",
    "    LA = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    A = LA(y_pred[:,0], y_true[:,0])\n",
    "        \n",
    "    LB = nn.MSELoss(reduction='none')\n",
    "    B = torch.sum(LB(y_pred[:,1:5], y_true[:,1:5]), dim = 1)\n",
    "    LC = nn.CrossEntropyLoss(reduction='none')\n",
    "    C = LC(y_pred[:,5:], y_true[:,5].long())\n",
    "    \n",
    "    #print(y_true[:,5])\n",
    "\n",
    "    \n",
    "        \n",
    "    #print(A.shape)\n",
    "    #print(B.shape)\n",
    "    #print(y_true.shape)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "        \n",
    "    L_loc = torch.where(y_true[:,0] == 1, A+B+C, A)\n",
    "    L_loc = torch.mean(L_loc)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"A\",A)\n",
    "    #print(\"B\",B)\n",
    "    #print(\"C\",C)\n",
    "    \n",
    "    \n",
    "    return L_loc\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3160ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f40fe072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:52:18.492397  |  Epoch 1  |  Training loss 2.513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.513425717302548]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = MNIST_model()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2) #lr=1e-2 is the same as lr=0.01\n",
    "\n",
    "loss_train = train(1, optimizer, model, loss_fn, train_loader)\n",
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "55767f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(bb1, bb2):\n",
    "    A_x0 = bb1[:,0] - bb1[:,2]/2\n",
    "    A_y0 = bb1[:,1] - bb1[:,3]/2\n",
    "    A_x1 = bb1[:,0] + bb1[:,2]/2\n",
    "    A_y1 = bb1[:,1] + bb1[:,3]/2\n",
    "    \n",
    "    B_x0 = bb2[:,0] - bb2[:,2]/2\n",
    "    B_y0 = bb2[:,1] - bb2[:,3]/2\n",
    "    B_x1 = bb2[:,0] + bb2[:,2]/2\n",
    "    B_y1 = bb2[:,1] + bb2[:,3]/2\n",
    "    \n",
    "    #compare each element of the tensors between A and B\n",
    "    \n",
    "    I_x0 = torch.max(A_x0,B_x0)\n",
    "    I_y0 = torch.max(A_y0,B_y0)\n",
    "    I_x1 = torch.min(A_x1,B_x1)\n",
    "    I_y1 = torch.min(A_y1,B_y1)\n",
    "    \n",
    "    I_area = (I_x1 - I_x0)*(I_y1 - I_y0)\n",
    "    \n",
    "    A_area = (A_x1 - A_x0)*(A_y1 - A_y0)\n",
    "    B_area = (B_x1 - B_x0)*(B_y1 - B_y0)\n",
    "    \n",
    "    U_area = A_area + B_area - I_area\n",
    "    \n",
    "    IoU = I_area / U_area\n",
    "    \n",
    "    return IoU\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44911ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_iou([1,1,2,1,2], [1,2,4,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9381d3c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2999646975.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/tw/y8qpz0xd4b39clr4jk675j_80000gn/T/ipykernel_70948/2999646975.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def plot_boundingbox\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def plot_boundingbox:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "335e47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tot_iou = 0\n",
    "    totalpc1 = 0\n",
    "    \n",
    "    totalpcright = 0\n",
    "\n",
    "    # We do not want gradients here, as we will not want to update the parameters.\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            if labels[0] == 0:\n",
    "                if outputs[0] == 0:\n",
    "                    correct += 1\n",
    "            else:\n",
    "                if outputs[0] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    if argmax(outputs[6:16]) == label[6]\n",
    "                        correct += 1\n",
    "                total_iou = iou(output[1:5], label[1:5]) \n",
    "                \n",
    "                correct/ba\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            pc1 = torch.where(labels[:,0] == 1)[0]\n",
    "            #print(pc1)\n",
    "            \n",
    "            \n",
    "            _, predicted = torch.max(outputs[:,5:], dim=1) \n",
    "            predicted = torch.unsqueeze(predicted, 1)\n",
    "            #print(labels.shape[0])\n",
    "            total += labels.shape[0]\n",
    "            totalpc1 += pc1.shape[0]\n",
    "            correct += int((predicted[pc1] == labels[pc1,5:]).sum())\n",
    "        \n",
    "            \n",
    "            #print(f\"{outputs[:,1:5].shape} {labels[:,1:5].shape}\")\n",
    "        \n",
    "            iou = compute_iou(outputs[pc1,1:5], labels[pc1,1:5])\n",
    "            \n",
    "            correctpc = predicted[:,0] == labels[:,0]\n",
    "            totalpcright += correctpc.shape[0]\n",
    "            \n",
    "            print(total.shape[0])\n",
    "            \n",
    "\n",
    "    accpc = totalpcright/total\n",
    "    print(accpc)\n",
    "\n",
    "    acc =  ((correct / total) + (iou/totalpc1))/2\n",
    "\n",
    "    #print(acc.shape)\n",
    "\n",
    "    #print(\"Accuracy: {:.2f}\".format(sum(acc)/acc.shape[0]))\n",
    "          \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b6eebfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:57:51.094225  |  Epoch 1  |  Training loss 2.415\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([128, 4]) torch.Size([128, 4])\n",
      "torch.Size([72, 4]) torch.Size([72, 4])\n",
      "torch.Size([72])\n",
      "Accuracy: 0.05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0451, 0.0448, 0.0448, 0.0448, 0.0454, 0.0448, 0.0452, 0.0449, 0.0449,\n",
       "        0.0449, 0.0449, 0.0448, 0.0452, 0.0448, 0.0449, 0.0447, 0.0448, 0.0451,\n",
       "        0.0450, 0.0389, 0.0449, 0.0448, 0.0451, 0.0448, 0.0448, 0.0455, 0.0456,\n",
       "        0.0447, 0.0447, 0.0449, 0.0449, 0.0448, 0.0445, 0.0448, 0.0446, 0.0449,\n",
       "        0.0444, 0.0449, 0.0453, 0.0450, 0.0447, 0.0449, 0.0449, 0.0449, 0.0449,\n",
       "        0.0447, 0.0449, 0.0441, 0.0450, 0.0449, 0.0450, 0.0448, 0.0449, 0.0450,\n",
       "        0.0449, 0.0451, 0.0451, 0.0443, 0.0449, 0.3895, 0.0447, 0.0448, 0.0448,\n",
       "        0.0450, 0.0433, 0.0449, 0.0485, 0.0449, 0.0449, 0.0452, 0.0448, 0.0448])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model_1 = MNIST_model()\n",
    "\n",
    "train(5, optimizer, model, loss_fn, train_loader)\n",
    "\n",
    "compute_accuracy(model_1, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b9c19779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tw/y8qpz0xd4b39clr4jk675j_80000gn/T/ipykernel_70948/4022386458.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/tw/y8qpz0xd4b39clr4jk675j_80000gn/T/ipykernel_70948/4226797697.py\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mtotalpcright\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrectpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "compute_accuracy(model_1, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df338c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ca810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
