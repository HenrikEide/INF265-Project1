{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from collections import Counter\n",
    "from utils import train, set_device, compute_accuracy\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Word embedding\n",
    "\n",
    "Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read txt file and tokenize\n",
    "def read_tokenize_txt(path):\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        tokens = nltk.tokenize.word_tokenize(f.read())\n",
    "    return tokens\n",
    "\n",
    "# Read all txt files in a directory and tokenize\n",
    "def read_tokenize_dir(path):\n",
    "    tokens = []\n",
    "    for file in os.listdir(path):\n",
    "        tokens += read_tokenize_txt(path + file) # Should use os.path.join\n",
    "    return tokens\n",
    "\n",
    "\n",
    "train_data = read_tokenize_dir('../data_train/')\n",
    "test_data = read_tokenize_dir('../data_test/')\n",
    "val_data = read_tokenize_dir('../data_val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training data: 2,757,691\n",
      "Number of distinct tokens in training data: 60,424\n",
      "Size of vocabulary: 2,177\n",
      "Comments:\n",
      "A little more than 3% of the tokens are in the vocabulary with the threshold of 100 occurences. This seems resonable.\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "\n",
    "def get_freq_vocab(data, min_freq=100):\n",
    "    freq = Counter(data)\n",
    "    vocab = {w:f for (w,f) in freq.items() if freq[w] >= min_freq}\n",
    "    return freq, vocab\n",
    "\n",
    "print(f\"Number of tokens in training data: {len(train_data):,}\")\n",
    "freq, vocab = get_freq_vocab(train_data, min_freq=100)\n",
    "print(f\"Number of distinct tokens in training data: {len(freq):,}\")\n",
    "print(f\"Size of vocabulary: {len(vocab):,}\")\n",
    "print(\"Comments:\\nA little more than 3% of the tokens are in the vocabulary with the threshold of 100 occurences. This seems resonable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "class My_MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16, context_size=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim*context_size)\n",
    "        # self.embedding.load_state_dict(self.embedding.state_dict())\n",
    "        self.fc1 = nn.Linear(emb_dim*context_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very much inspired by exercise 6. For this to work we need to convert the tokenized data into a format that the model can understand by creating a context/target dataset. The context is the sequence of words that surround the target word. The target word is the word we are trying to predict given context. We can then create a dataset that pairs the context window with the target word. The context window will be the input to the model, and the target word will be the target for the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(text, vocab, context_size=3):\n",
    "    \"\"\"Create a PyTorch dataset of context/target pairs from text\"\"\"\n",
    "    # Remove words that are not in the vocabulary\n",
    "    text = [w for w in text if w in vocab.keys()]\n",
    "\n",
    "    # Map each word to its index in the vocabulary\n",
    "    word_to_ix = {word: i for i, word in enumerate(vocab.keys())}\n",
    "    \n",
    "    # Transform the text as a list of integers.\n",
    "    data = [word_to_ix[word] for word in text]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(context_size, len(text) - context_size):\n",
    "        target = data[i]\n",
    "        context = data[i - context_size:i] + data[i + 1:i + context_size + 1]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "            \n",
    "    # Convert context/target lists to PyTorch tensor\n",
    "    context_tensor = torch.tensor(contexts)\n",
    "    target_tensor = torch.tensor(targets)\n",
    "\n",
    "    # Create a PyTorch dataset out of these context / target pairs\n",
    "    return TensorDataset(context_tensor, target_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------ Model1 ------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (6) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Henrik Olsen Eide\\UibEmner\\INF265\\INF265-Projects\\project3\\sequence_models.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Henrik%20Olsen%20Eide/UibEmner/INF265/INF265-Projects/project3/sequence_models.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Henrik%20Olsen%20Eide/UibEmner/INF265/INF265-Projects/project3/sequence_models.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# I got this far without using DataLoader, and now I am filled with nothing but regret and shame.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Henrik%20Olsen%20Eide/UibEmner/INF265/INF265-Projects/project3/sequence_models.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train(\u001b[39m30\u001b[39;49m, optimizer, model, loss_fn, data_train, \u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Henrik%20Olsen%20Eide/UibEmner/INF265/INF265-Projects/project3/sequence_models.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Henrik%20Olsen%20Eide/UibEmner/INF265/INF265-Projects/project3/sequence_models.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Compute accuracy on training and validation data sets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Henrik Olsen Eide\\UibEmner\\INF265\\INF265-Projects\\project3\\utils.py:33\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(n_epochs, optimizer, model, loss_fn, train_loader, device)\u001b[0m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m outputs \u001b[39m=\u001b[39m model(contexts)\n\u001b[1;32m---> 33\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, targets)\n\u001b[0;32m     34\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (6) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "# Part 4\n",
    "\n",
    "torch.manual_seed(265)\n",
    "# train_freq and train_vocab are already initialized in Part 2\n",
    "data_train = create_dataset(train_data, vocab)\n",
    "\n",
    "val_freq, val_vocab = get_freq_vocab(val_data)\n",
    "data_val = create_dataset(val_data, val_vocab)\n",
    "\n",
    "test_freq, test_vocab = get_freq_vocab(test_data)\n",
    "data_test = create_dataset(test_data, test_vocab)\n",
    "\n",
    "# embedding = nn.Embedding(len(vocab), 16)\n",
    "models = [My_MLP(len(vocab)), My_MLP(len(vocab)), My_MLP(len(vocab)), My_MLP(len(vocab))]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(model_name := f\"\\n------ Model{i+1} ------\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # I got this far without using DataLoader, and now I am filled with nothing but regret and shame.\n",
    "    train(30, optimizer, model, loss_fn, data_train, \"cpu\")\n",
    "    # The train function doesnt even use batch_size, but still that is what it crashes on.\n",
    "    torch.save(model.to(device=\"cpu\"), f\"models/{model_name}.pt\")\n",
    "    \n",
    "\n",
    "    # Compute accuracy on training and validation data sets\n",
    "    print(f\"Training accuracy: {compute_accuracy(model, data_train, 'cpu'):.2f}%\")\n",
    "    print(f\"Validation accuracy: {compute_accuracy(model, data_val, 'cpu'):.2f}%\")\n",
    "\n",
    "\n",
    "best_model = None # We dont know, because the code above is borked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5\n",
    "\n",
    "# Compute that cosine similarity matrix, if I can get the model to work\n",
    "embedding = nn.Embedding(len(vocab), 16)\n",
    "embedding.load_state_dict(torch.load(\"models/my_404_embedding.pt\"))\n",
    "embedding_weights = embedding.weight.data\n",
    "cos_sim_matrix = torch.nn.functional.cosine_similarity(embedding_weights, embedding_weights, dim=1)\n",
    "\n",
    "# Reporting similar words for some random words\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab.keys())}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab.keys())}\n",
    "words = [\"man\", \"be\", \"have\", \"how\", \"castle\"]\n",
    "word_indices = [word_to_ix[word] for word in words]\n",
    "\n",
    "for word, word_index in zip(words, word_indices):\n",
    "    similarity_row = cos_sim_matrix[word_index]\n",
    "    # Sort the row, without the word itself\n",
    "    sorted_row = similarity_row.argsort(descending=True)[1:][:10]\n",
    "    top_words = [ix_to_word[index.item()] for index in sorted_row]\n",
    "    print(f\"\\nMost similar to '{word}': {', '.join(top_words)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Conjugating *be* and *have*\n",
    "\n",
    "First we define the architectures, MLP, MLP with attention and RNN, using LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.fc1 = nn.Linear(embedding_matrix.shape[1], hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1)  # average the embeddings across the context\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MLPA(nn.Module): # A as in attention\n",
    "    def __init__(self, embedding_matrix, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.attention = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "        self.fc1 = nn.Linear(embedding_matrix.shape[1], hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        alpha = F.softmax(self.attention(x), dim=1)\n",
    "        x = alpha * x\n",
    "        x = x.sum(dim=1)  # weighted sum of the embeddings based on attention\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.rnn = nn.LSTM(embedding_matrix.shape[1], hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h_n, _) = self.rnn(x)\n",
    "        x = h_n.squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation\n",
    "\n",
    "models = [MLP(embedding_weights), MLPA(embedding_weights), RNN(embedding_weights)]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(model_name := f\"\\n------ Model{i+1} ------\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    train(30, optimizer, model, loss_fn, data_train, \"cpu\")\n",
    "    torch.save(model.to(device=\"cpu\"), f\"models/{model_name}.pt\")\n",
    "\n",
    "    # TODO: Only use the datasets that have the targets:\n",
    "    # \"be, am, are, is, was, were, been, being, have, has, had, having\"\n",
    "    print(f\"Training accuracy: {compute_accuracy(model, data_train, 'cpu'):.2f}%\")\n",
    "    print(f\"Validation accuracy: {compute_accuracy(model, data_val, 'cpu'):.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
