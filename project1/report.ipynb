{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the function used for computing backpropogating of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "def backpropagation(model, y_true, y_pred):\n",
    "    l = model.L\n",
    "    ed = -2*(y_true-y_pred)\n",
    "    delta = ed*(model.df[l](model.z[l]))\n",
    "    pdw = delta.T*(model.a[l-1])\n",
    "    model.dL_dw[l] = pdw\n",
    "    model.dL_db[l] = torch.flatten(delta)\n",
    "    for layer in range(l-1, 0, -1):\n",
    "        weights = model.fc[str(layer+1)].weight.data\n",
    "        delta = (delta@weights)*(model.df[layer](model.z[layer]))\n",
    "        pdw = delta.T*(model.a[layer-1])\n",
    "        model.dL_dw[layer] = pdw\n",
    "        model.dL_db[layer] = torch.flatten(delta)\n",
    "\n",
    "    return None\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n",
    "The function iterates through the layers in reverse order, computing the delta for each layer using the delta of the next layer and the weights of the current layer, and then computes the gradients of the loss function with respect to the weights and biases of the current layer, and stores them in the `dL_dw` and `dL_db` in the `model`.\n",
    "\n",
    "\n",
    "The delta variable represents the error signal propagated backwards through the network, and the `pdw` variable is used to store the partial derivative of the loss function with respect to the `weights` of each layer. We use matrix multiplication to compute the gradients for each layer. Additionally, the `flatten` function converts the bias tensor into a one-dimensional tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "### a)\n",
    "Our backpropagation implementation corresponds for example to the PyTorch methods `loss.backward()` and `torch.autograd.grad()`. These methods can be used to compute the gradients of the model parameters with respect to the loss function, which can then be used to update the parameters with the optimizer. \n",
    "\n",
    "### b)\n",
    "To check whether the computed gradient of a function seems correct we use numerical gradient checking. The method compares the numerically approximated gradients using finite difference method with the gradients computed by backpropagation. If the numerical gradients and the backpropagation gradients are close, then it is likely that the gradients are correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
