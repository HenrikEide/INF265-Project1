{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    l = model.L\n",
    "    ed = -2*(y_true-y_pred)\n",
    "    delta = ed*(model.df[l](model.z[l]))\n",
    "    pdw = delta.T*(model.a[l-1])\n",
    "    model.dL_dw[l] = pdw\n",
    "    model.dL_db[l] = torch.flatten(delta)\n",
    "    for layer in range(l-1, 0, -1):\n",
    "        weights = model.fc[str(layer+1)].weight.data\n",
    "        delta = (delta@weights)*(model.df[layer](model.z[layer]))\n",
    "        pdw = delta.T*(model.a[layer-1])\n",
    "        model.dL_dw[layer] = pdw\n",
    "        model.dL_db[layer] = delta[0]\n",
    "        print(delta)\n",
    "       # print(\"Hello\")\n",
    "        #print(torch.flatten(delta))\n",
    "        #print(delta[0])\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.1875,  0.0510,  0.2081]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1250,  0.8498, -0.8401]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0298,  0.0606, -0.0674]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0844, -0.1868,  0.0285]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0110, -0.0308,  0.0119]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0037, -0.0012,  0.0067]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0013,  0.0196,  0.0033]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0007,  0.0084,  0.0028]], grad_fn=<MulBackward0>)\n",
      "tensor([[-6.0178e-05,  3.3598e-02,  3.9726e-04]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5.5591e-05,  5.8525e-02,  1.6770e-04]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.5447, -0.0538, -0.6074]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0623,  0.0092,  0.0229]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0117,  0.0014,  0.0018]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0059,  0.0029,  0.0063]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0010,  0.0061,  0.0014]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0004,  0.0053,  0.0007]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0001,  0.0060,  0.0004]], grad_fn=<MulBackward0>)\n",
      "tensor([[-7.9232e-05,  2.4988e-03,  3.1386e-04]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5.3138e-06,  2.9485e-03,  3.4782e-05]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5.2979e-06,  3.5491e-02,  1.6446e-05]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.4614, -0.0873, -0.6951]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0702,  0.0145,  0.0283]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0068,  0.0030,  0.0010]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0020,  0.0020,  0.0018]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0003,  0.0034,  0.0003]], grad_fn=<MulBackward0>)\n",
      "tensor([[-9.1200e-05,  2.3740e-03,  1.3744e-04]], grad_fn=<MulBackward0>)\n",
      "tensor([[-3.2175e-05,  2.3087e-03,  5.8471e-05]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.7477e-05,  8.7379e-04,  4.6248e-05]], grad_fn=<MulBackward0>)\n",
      "tensor([[-9.0286e-07,  7.2810e-04,  3.5604e-06]], grad_fn=<MulBackward0>)\n",
      "tensor([[-8.8731e-07,  1.0686e-02,  1.7554e-06]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.2352, -0.0499, -0.4768]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0660,  0.0149,  0.0290]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0048,  0.0036,  0.0007]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0010,  0.0015,  0.0008]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0001,  0.0022,  0.0001]], grad_fn=<MulBackward0>)\n",
      "tensor([[-4.0979e-05,  1.4679e-03,  4.4422e-05]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.3713e-05,  1.3609e-03,  1.7096e-05]], grad_fn=<MulBackward0>)\n",
      "tensor([[-7.2535e-06,  5.0530e-04,  1.2560e-05]], grad_fn=<MulBackward0>)\n",
      "tensor([[-3.2286e-07,  3.7010e-04,  7.4407e-07]], grad_fn=<MulBackward0>)\n",
      "tensor([[-3.1800e-07,  5.5255e-03,  3.8472e-07]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.1045, -0.0254, -0.2540]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0563,  0.0132,  0.0262]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0037,  0.0038,  0.0006]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.0007,  0.0011,  0.0005]], grad_fn=<MulBackward0>)\n",
      "tensor([[-8.5346e-05,  1.5954e-03,  6.1807e-05]], grad_fn=<MulBackward0>)\n",
      "tensor([[-2.5461e-05,  1.0242e-03,  2.2856e-05]], grad_fn=<MulBackward0>)\n",
      "tensor([[-8.2869e-06,  9.2510e-04,  8.3203e-06]], grad_fn=<MulBackward0>)\n",
      "tensor([[-4.3384e-06,  3.4089e-04,  5.8758e-06]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.7862e-07,  2.3180e-04,  3.0137e-07]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.7450e-07,  3.4619e-03,  1.5931e-07]], grad_fn=<MulBackward0>)\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=False, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "tensor([[ 4.2234,  5.7877,  6.8000, -5.1739, -4.8914, -3.9720, -1.8875,  3.2703,\n",
      "         -1.1382, -2.9207, -3.0426,  0.5908, -1.7191,  8.2706, -0.1163,  1.1097]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Hello\n",
      "tensor([ 4.2234,  5.7877,  6.8000, -5.1739, -4.8914, -3.9720, -1.8875,  3.2703,\n",
      "        -1.1382, -2.9207, -3.0426,  0.5908, -1.7191,  8.2706, -0.1163,  1.1097],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristiansvendsen/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.4977e-04,  5.4455e-06, -5.0843e-06, -1.2099e-03,  3.4277e-03,\n",
      "         -6.9010e-02, -4.8888e-02, -6.1763e-04, -4.2158e-01, -1.8283e-02,\n",
      "          1.4199e-02, -6.5599e-01,  9.3027e-02, -4.3346e-06, -1.6800e-01,\n",
      "         -4.7703e-01]], grad_fn=<MulBackward0>)\n",
      "Hello\n",
      "tensor([ 5.4977e-04,  5.4455e-06, -5.0843e-06, -1.2099e-03,  3.4277e-03,\n",
      "        -6.9010e-02, -4.8888e-02, -6.1763e-04, -4.2158e-01, -1.8283e-02,\n",
      "         1.4199e-02, -6.5599e-01,  9.3027e-02, -4.3346e-06, -1.6800e-01,\n",
      "        -4.7703e-01], grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([[ 6.2709e-21,  2.0907e-28, -1.0523e-31, -6.2003e-25,  2.1973e-24,\n",
      "         -2.4942e-20, -7.9916e-13, -3.8823e-17, -9.1087e-11, -9.1539e-17,\n",
      "          2.5487e-16, -3.0564e-06,  6.7951e-11, -1.6967e-37, -5.0003e-06,\n",
      "         -3.2246e-06]], grad_fn=<MulBackward0>)\n",
      "Hello\n",
      "tensor([ 6.2709e-21,  2.0907e-28, -1.0523e-31, -6.2003e-25,  2.1973e-24,\n",
      "        -2.4942e-20, -7.9916e-13, -3.8823e-17, -9.1087e-11, -9.1539e-17,\n",
      "         2.5487e-16, -3.0564e-06,  6.7951e-11, -1.6967e-37, -5.0003e-06,\n",
      "        -3.2246e-06], grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([[ -2.3100e-87, -3.7510e-121,  2.0453e-138,  4.7533e-107, -3.2403e-101,\n",
      "           2.1944e-83,   6.5750e-42,   2.4135e-68,   1.9001e-34,   1.4731e-61,\n",
      "          -2.7114e-63,   3.1646e-05,  -3.9880e-34,  1.2672e-168,   1.0735e-09,\n",
      "           3.5026e-11]], grad_fn=<MulBackward0>)\n",
      "Hello\n",
      "tensor([ -2.3100e-87, -3.7510e-121,  2.0453e-138,  4.7533e-107, -3.2403e-101,\n",
      "          2.1944e-83,   6.5750e-42,   2.4135e-68,   1.9001e-34,   1.4731e-61,\n",
      "         -2.7114e-63,   3.1646e-05,  -3.9880e-34,  1.2672e-168,   1.0735e-09,\n",
      "          3.5026e-11], grad_fn=<ReshapeAliasBackward0>)\n",
      "tensor([[  3.1034e-50,   6.5346e-72,  -2.0070e-81,  -5.9696e-62,   3.4784e-59,\n",
      "          -7.6719e-48,  -9.5872e-23,  -3.3809e-39,  -1.1478e-11,  -6.8526e-35,\n",
      "           2.3639e-36,  -3.3672e-08,   2.3153e-21, -5.9428e-101,  -2.2778e-01,\n",
      "          -1.2226e-13]], grad_fn=<MulBackward0>)\n",
      "Hello\n",
      "tensor([  3.1034e-50,   6.5346e-72,  -2.0070e-81,  -5.9696e-62,   3.4784e-59,\n",
      "         -7.6719e-48,  -9.5872e-23,  -3.3809e-39,  -1.1478e-11,  -6.8526e-35,\n",
      "          2.3639e-36,  -3.3672e-08,   2.3153e-21, -5.9428e-101,  -2.2778e-01,\n",
      "         -1.2226e-13], grad_fn=<ReshapeAliasBackward0>)\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=False, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "fc1f7c21504a34dc0c745a9195bd709da13cda6a059cd46dc7fea8e2a656e7c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
